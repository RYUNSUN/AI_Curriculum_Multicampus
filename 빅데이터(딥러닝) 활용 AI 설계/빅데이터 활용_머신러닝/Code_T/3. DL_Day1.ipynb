{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])\n",
    "w1=tf.get_variable(\"w1\", shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([256]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\", shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "hf= tf.matmul(L2,w3)+b3\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "               (logits=hf, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.307204902\n",
      "cost: 0.118177805\n",
      "cost: 0.075792339\n",
      "cost: 0.055236486\n",
      "cost: 0.041654695\n",
      "cost: 0.030583053\n",
      "cost: 0.025787624\n",
      "cost: 0.019321068\n",
      "cost: 0.015056813\n",
      "cost: 0.015982657\n",
      "cost: 0.015713583\n",
      "cost: 0.013116931\n",
      "cost: 0.011356083\n",
      "cost: 0.006876326\n",
      "cost: 0.009233987\n",
      "acc: 0.977\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(hf, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "w1=tf.get_variable(\"w1\", shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "\n",
    "w2=tf.get_variable(\"w2\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "\n",
    "w4=tf.get_variable(\"w4\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "\n",
    "w5=tf.get_variable(\"w5\", shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "               (logits=hf, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.301833063\n",
      "cost: 0.108337819\n",
      "cost: 0.070312861\n",
      "cost: 0.057052072\n",
      "cost: 0.044098418\n",
      "cost: 0.034306396\n",
      "cost: 0.034026378\n",
      "cost: 0.026596244\n",
      "cost: 0.023591126\n",
      "cost: 0.019954823\n",
      "cost: 0.018181242\n",
      "cost: 0.019435807\n",
      "cost: 0.016651686\n",
      "cost: 0.018277372\n",
      "cost: 0.013068188\n",
      "acc: 0.9773\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(hf, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 11:33:02.711234  4792 deprecation.py:506] From <ipython-input-24-828d1abeed02>:10: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "x=tf.placeholder(tf.float32, [None,784])\n",
    "y=tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "w1=tf.get_variable(\"w1\", shape=[784,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([512]))\n",
    "L1=tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "L1=tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "w2=tf.get_variable(\"w2\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([512]))\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)\n",
    "L2=tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([512]))\n",
    "L3=tf.nn.relu(tf.matmul(L2,w3)+b3)\n",
    "L3=tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "w4=tf.get_variable(\"w4\", shape=[512,512],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([512]))\n",
    "L4=tf.nn.relu(tf.matmul(L3,w4)+b4)\n",
    "L4=tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "w5=tf.get_variable(\"w5\", shape=[512,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5=tf.Variable(tf.random_normal([10]))\n",
    "hf=tf.matmul(L4,w5)+b5\n",
    "\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n",
    "               (logits=hf, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.471288317\n",
      "cost: 0.170179785\n",
      "cost: 0.130676515\n",
      "cost: 0.106415906\n",
      "cost: 0.096071292\n",
      "cost: 0.084453547\n",
      "cost: 0.074357380\n",
      "cost: 0.068989190\n",
      "cost: 0.063918490\n",
      "cost: 0.059588612\n",
      "cost: 0.058148802\n",
      "cost: 0.052497379\n",
      "cost: 0.047516165\n",
      "cost: 0.048972616\n",
      "cost: 0.046203336\n",
      "acc: 0.9838\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys, keep_prob:0.7}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(hf, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels,\n",
    "                       keep_prob:1}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist model based CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #그래프에 있는 모든 텐서를 초기화\n",
    "\n",
    "x=tf.placeholder(tf.float32, [None, 28*28])\n",
    "ximg=tf.reshape(x, [-1, 28, 28, 1])#img 28*28*1(black/white)\n",
    "y=tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "w1=tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "L1=tf.nn.conv2d(ximg, w1, strides=[1,1,1,1],padding='SAME')\n",
    "L1=tf.nn.relu(L1)\n",
    "L1=tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "#L1 이미지 shape => (?, 28, 28, 1)\n",
    "#conv => (?,28,28,32)\n",
    "#pooling -> (?,14,14,32)\n",
    "\n",
    "w2=tf.Variable(tf.random_normal([3,3,32,64]))\n",
    "L2=tf.nn.conv2d(L1, w2, strides=[1,1,1,1],padding='SAME')\n",
    "L2=tf.nn.relu(L2)\n",
    "L2=tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "#conv2d (?,14,14,64)\n",
    "#relu (?,14,14,64)\n",
    "#max_pool (?,7,7,64)\n",
    "L2_flat=tf.reshape(L2, [-1, 7*7*64])\n",
    "#reshape => (?, 7*7*64)\n",
    "\n",
    "\n",
    "w3=tf.get_variable(\"w3\", shape=[7*7*64, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b=tf.Variable(tf.random_normal([10]))\n",
    "logits=tf.matmul(L2_flat, w3)+b\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 1.063276401\n",
      "cost: 0.263196674\n",
      "cost: 0.159252752\n",
      "cost: 0.110893273\n",
      "cost: 0.082579888\n",
      "cost: 0.063731424\n",
      "cost: 0.051189612\n",
      "cost: 0.041525801\n",
      "cost: 0.036523743\n",
      "cost: 0.028419882\n",
      "cost: 0.022158340\n",
      "cost: 0.025332208\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batchxs,batchys=mnist.train.next_batch(batch_size)\n",
    "        myfeed={x:batchxs, y:batchys}\n",
    "        cv,_=sess.run([cost, train], feed_dict=myfeed)\n",
    "        avg_cost+=cv/total_batch\n",
    "    print('cost:','{:.9f}'.format(avg_cost))\n",
    "\n",
    "c_pre=tf.equal(tf.argmax(logits, 1), tf.argmax(y,1))\n",
    "acc=tf.reduce_mean(tf.cast(c_pre, tf.float32))\n",
    "print('acc:', sess.run(acc, feed_dict=\n",
    "                       {x:mnist.test.images, \n",
    "                        y:mnist.test.labels,\n",
    "                       }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
