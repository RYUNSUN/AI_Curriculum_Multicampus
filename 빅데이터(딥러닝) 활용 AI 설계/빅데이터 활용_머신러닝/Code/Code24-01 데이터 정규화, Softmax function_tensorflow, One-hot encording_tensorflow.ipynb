{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 변환\n",
    "(1) 표준화 (이상치처리)  \n",
    "(2) 정규화 (0~1)  \n",
    "(3) 이산화(Binarizer): 연속형 -> 이항변수, (Binarizer, OneHot Encoding)  \n",
    "(4) 변수 개수 축소(feature selection)  \n",
    "(5) 차원축소 (PCA)    \n",
    "(6) 시그널 변환(퓨리에변환, 웨이블릿 변환)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"axes.facecolor\":\"white\", \"axes.edgecolor\":\"black\"})\n",
    "import matplotlib as mpl\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정규화 전/후 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 맨 마지막 열이 label\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling 식 직접 쓰기\n",
    "# 타겟은 scaling 할 필요 없음\n",
    "def MyScaler(data): #MinMaxScaling\n",
    "    # print(np.mIN(data)) # 전체데이터에서 최소값\n",
    "    de=np.max(data, axis=0)-np.min(data,axis=0) # 열 단위 최소값 (각 feature를 정규화해야하기 때문에 열 단위로)\n",
    "    num = data-np.min(data,axis=0) # 각 데이터에서 해당 열의 최소값을 뺸 값 출력\n",
    "    return num/de\n",
    "    # print(np.min(data, axis=1)) # 행 단위 최소값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881783 0.83755792]\n",
      " [0.54412549 0.50274824 0.57608696 0.60646801 0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.4258239  0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.4258239  0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 정규화\n",
    "xy=MyScaler(xy)\n",
    "# xy.shape #(8, 5)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata=xy[:,0:-1] # 맨 마지막 열을 제외한 열들 추출\n",
    "xdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata=xy[:,[-1]] # 맨 마지막 열 추출\n",
    "ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,4])\n",
    "y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "w=tf.Variable(tf.random_normal([4,1]), name=\"weight\")\n",
    "b=tf.Variable(tf.random_normal([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.matmul(x,w)+b\n",
    "cost=tf.reduce_mean(tf.square(hf-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.train.GradientDescentOptimizer(1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 1.5128553 hf: [[-1.1081699 ]\n",
      " [-1.0915207 ]\n",
      " [-0.6612911 ]\n",
      " [-0.1562188 ]\n",
      " [-0.49465302]\n",
      " [-0.49340346]\n",
      " [ 0.1430142 ]\n",
      " [ 0.11878243]]\n",
      "1 cost: 1.5127568 hf: [[-1.1081103 ]\n",
      " [-1.0914645 ]\n",
      " [-0.66124475]\n",
      " [-0.15618348]\n",
      " [-0.49461055]\n",
      " [-0.4933626 ]\n",
      " [ 0.14303996]\n",
      " [ 0.11880726]]\n",
      "2 cost: 1.5126586 hf: [[-1.1080507 ]\n",
      " [-1.0914084 ]\n",
      " [-0.6611985 ]\n",
      " [-0.15614817]\n",
      " [-0.49456808]\n",
      " [-0.49332175]\n",
      " [ 0.14306575]\n",
      " [ 0.1188321 ]]\n",
      "3 cost: 1.51256 hf: [[-1.1079912 ]\n",
      " [-1.0913522 ]\n",
      " [-0.6611522 ]\n",
      " [-0.15611285]\n",
      " [-0.4945256 ]\n",
      " [-0.49328095]\n",
      " [ 0.1430915 ]\n",
      " [ 0.11885694]]\n",
      "4 cost: 1.5124615 hf: [[-1.1079315 ]\n",
      " [-1.091296  ]\n",
      " [-0.66110575]\n",
      " [-0.15607753]\n",
      " [-0.49448314]\n",
      " [-0.4932401 ]\n",
      " [ 0.14311728]\n",
      " [ 0.11888176]]\n",
      "5 cost: 1.5123632 hf: [[-1.107872  ]\n",
      " [-1.0912399 ]\n",
      " [-0.6610594 ]\n",
      " [-0.15604222]\n",
      " [-0.49444067]\n",
      " [-0.49319923]\n",
      " [ 0.14314304]\n",
      " [ 0.1189066 ]]\n",
      "6 cost: 1.5122647 hf: [[-1.1078123 ]\n",
      " [-1.0911838 ]\n",
      " [-0.661013  ]\n",
      " [-0.1560069 ]\n",
      " [-0.4943982 ]\n",
      " [-0.49315837]\n",
      " [ 0.14316879]\n",
      " [ 0.11893143]]\n",
      "7 cost: 1.512166 hf: [[-1.1077528 ]\n",
      " [-1.0911274 ]\n",
      " [-0.66096646]\n",
      " [-0.15597159]\n",
      " [-0.49435574]\n",
      " [-0.4931175 ]\n",
      " [ 0.14319456]\n",
      " [ 0.11895627]]\n",
      "8 cost: 1.5120678 hf: [[-1.1076932 ]\n",
      " [-1.0910714 ]\n",
      " [-0.66092014]\n",
      " [-0.15593627]\n",
      " [-0.49431333]\n",
      " [-0.49307665]\n",
      " [ 0.1432203 ]\n",
      " [ 0.11898109]]\n",
      "9 cost: 1.5119693 hf: [[-1.1076336 ]\n",
      " [-1.0910152 ]\n",
      " [-0.6608738 ]\n",
      " [-0.15590096]\n",
      " [-0.49427086]\n",
      " [-0.49303585]\n",
      " [ 0.14324605]\n",
      " [ 0.11900593]]\n",
      "10 cost: 1.511871 hf: [[-1.107574  ]\n",
      " [-1.0909591 ]\n",
      " [-0.6608275 ]\n",
      " [-0.15586558]\n",
      " [-0.49422845]\n",
      " [-0.492995  ]\n",
      " [ 0.14327185]\n",
      " [ 0.11903076]]\n",
      "11 cost: 1.5117726 hf: [[-1.1075144 ]\n",
      " [-1.0909028 ]\n",
      " [-0.6607811 ]\n",
      " [-0.15583026]\n",
      " [-0.49418592]\n",
      " [-0.4929542 ]\n",
      " [ 0.1432976 ]\n",
      " [ 0.1190556 ]]\n",
      "12 cost: 1.511674 hf: [[-1.1074548 ]\n",
      " [-1.0908467 ]\n",
      " [-0.66073465]\n",
      " [-0.15579489]\n",
      " [-0.49414346]\n",
      " [-0.49291328]\n",
      " [ 0.14332336]\n",
      " [ 0.11908044]]\n",
      "13 cost: 1.5115757 hf: [[-1.1073953 ]\n",
      " [-1.0907905 ]\n",
      " [-0.6606883 ]\n",
      " [-0.15575963]\n",
      " [-0.49410105]\n",
      " [-0.49287248]\n",
      " [ 0.14334914]\n",
      " [ 0.11910526]]\n",
      "14 cost: 1.5114772 hf: [[-1.1073357 ]\n",
      " [-1.0907344 ]\n",
      " [-0.660642  ]\n",
      " [-0.15572432]\n",
      " [-0.49405858]\n",
      " [-0.49283156]\n",
      " [ 0.14337489]\n",
      " [ 0.1191301 ]]\n",
      "15 cost: 1.5113789 hf: [[-1.107276  ]\n",
      " [-1.0906782 ]\n",
      " [-0.6605956 ]\n",
      " [-0.15568894]\n",
      " [-0.4940161 ]\n",
      " [-0.49279076]\n",
      " [ 0.14340065]\n",
      " [ 0.11915493]]\n",
      "16 cost: 1.5112804 hf: [[-1.1072165 ]\n",
      " [-1.0906221 ]\n",
      " [-0.66054904]\n",
      " [-0.15565369]\n",
      " [-0.49397358]\n",
      " [-0.49274984]\n",
      " [ 0.14342642]\n",
      " [ 0.11917977]]\n",
      "17 cost: 1.5111821 hf: [[-1.1071569 ]\n",
      " [-1.0905658 ]\n",
      " [-0.6605028 ]\n",
      " [-0.15561837]\n",
      " [-0.49393117]\n",
      " [-0.49270904]\n",
      " [ 0.14345217]\n",
      " [ 0.1192046 ]]\n",
      "18 cost: 1.5110836 hf: [[-1.1070973 ]\n",
      " [-1.0905097 ]\n",
      " [-0.6604564 ]\n",
      " [-0.155583  ]\n",
      " [-0.4938887 ]\n",
      " [-0.49266824]\n",
      " [ 0.14347792]\n",
      " [ 0.11922944]]\n",
      "19 cost: 1.5109854 hf: [[-1.1070378 ]\n",
      " [-1.0904536 ]\n",
      " [-0.66041   ]\n",
      " [-0.15554768]\n",
      " [-0.4938463 ]\n",
      " [-0.49262738]\n",
      " [ 0.14350371]\n",
      " [ 0.11925426]]\n",
      "20 cost: 1.5108871 hf: [[-1.1069782 ]\n",
      " [-1.0903974 ]\n",
      " [-0.66036373]\n",
      " [-0.15551245]\n",
      " [-0.4938038 ]\n",
      " [-0.4925865 ]\n",
      " [ 0.14352943]\n",
      " [ 0.11927907]]\n",
      "21 cost: 1.5107887 hf: [[-1.1069186 ]\n",
      " [-1.0903411 ]\n",
      " [-0.66031736]\n",
      " [-0.1554771 ]\n",
      " [-0.49376136]\n",
      " [-0.49254572]\n",
      " [ 0.1435552 ]\n",
      " [ 0.11930388]]\n",
      "22 cost: 1.5106905 hf: [[-1.106859  ]\n",
      " [-1.0902851 ]\n",
      " [-0.660271  ]\n",
      " [-0.15544182]\n",
      " [-0.49371898]\n",
      " [-0.4925049 ]\n",
      " [ 0.14358091]\n",
      " [ 0.11932868]]\n",
      "23 cost: 1.510592 hf: [[-1.1067994 ]\n",
      " [-1.090229  ]\n",
      " [-0.66022474]\n",
      " [-0.15540648]\n",
      " [-0.49367654]\n",
      " [-0.49246407]\n",
      " [ 0.14360663]\n",
      " [ 0.11935349]]\n",
      "24 cost: 1.510494 hf: [[-1.10674   ]\n",
      " [-1.090173  ]\n",
      " [-0.66017836]\n",
      " [-0.15537119]\n",
      " [-0.49363416]\n",
      " [-0.49242324]\n",
      " [ 0.14363237]\n",
      " [ 0.11937828]]\n",
      "25 cost: 1.5103955 hf: [[-1.1066804 ]\n",
      " [-1.0901167 ]\n",
      " [-0.6601319 ]\n",
      " [-0.15533596]\n",
      " [-0.49359173]\n",
      " [-0.49238247]\n",
      " [ 0.1436581 ]\n",
      " [ 0.11940309]]\n",
      "26 cost: 1.5102972 hf: [[-1.1066208 ]\n",
      " [-1.0900605 ]\n",
      " [-0.6600856 ]\n",
      " [-0.15530068]\n",
      " [-0.4935493 ]\n",
      " [-0.49234164]\n",
      " [ 0.14368382]\n",
      " [ 0.11942789]]\n",
      "27 cost: 1.5101991 hf: [[-1.1065612 ]\n",
      " [-1.0900044 ]\n",
      " [-0.66003937]\n",
      " [-0.15526539]\n",
      " [-0.49350685]\n",
      " [-0.49230087]\n",
      " [ 0.14370954]\n",
      " [ 0.11945269]]\n",
      "28 cost: 1.5101008 hf: [[-1.1065018 ]\n",
      " [-1.0899484 ]\n",
      " [-0.659993  ]\n",
      " [-0.1552301 ]\n",
      " [-0.4934644 ]\n",
      " [-0.49226004]\n",
      " [ 0.14373529]\n",
      " [ 0.1194775 ]]\n",
      "29 cost: 1.5100025 hf: [[-1.1064422 ]\n",
      " [-1.0898921 ]\n",
      " [-0.6599466 ]\n",
      " [-0.15519482]\n",
      " [-0.49342197]\n",
      " [-0.49221927]\n",
      " [ 0.14376101]\n",
      " [ 0.11950229]]\n",
      "30 cost: 1.5099041 hf: [[-1.1063826 ]\n",
      " [-1.0898361 ]\n",
      " [-0.65990037]\n",
      " [-0.15515953]\n",
      " [-0.4933796 ]\n",
      " [-0.49217844]\n",
      " [ 0.14378676]\n",
      " [ 0.1195271 ]]\n",
      "31 cost: 1.5098062 hf: [[-1.1063232 ]\n",
      " [-1.0897801 ]\n",
      " [-0.659854  ]\n",
      " [-0.15512425]\n",
      " [-0.49333715]\n",
      " [-0.4921376 ]\n",
      " [ 0.14381249]\n",
      " [ 0.1195519 ]]\n",
      "32 cost: 1.5097077 hf: [[-1.1062636 ]\n",
      " [-1.0897238 ]\n",
      " [-0.6598076 ]\n",
      " [-0.15508896]\n",
      " [-0.49329478]\n",
      " [-0.49209678]\n",
      " [ 0.14383821]\n",
      " [ 0.11957671]]\n",
      "33 cost: 1.5096097 hf: [[-1.1062043 ]\n",
      " [-1.0896678 ]\n",
      " [-0.65976137]\n",
      " [-0.15505373]\n",
      " [-0.4932524 ]\n",
      " [-0.49205607]\n",
      " [ 0.14386393]\n",
      " [ 0.1196015 ]]\n",
      "34 cost: 1.5095115 hf: [[-1.1061447 ]\n",
      " [-1.0896118 ]\n",
      " [-0.659715  ]\n",
      " [-0.15501839]\n",
      " [-0.4932099 ]\n",
      " [-0.49201524]\n",
      " [ 0.14388965]\n",
      " [ 0.11962631]]\n",
      "35 cost: 1.509413 hf: [[-1.1060851 ]\n",
      " [-1.0895555 ]\n",
      " [-0.6596686 ]\n",
      " [-0.1549831 ]\n",
      " [-0.49316752]\n",
      " [-0.4919744 ]\n",
      " [ 0.14391539]\n",
      " [ 0.11965111]]\n",
      "36 cost: 1.5093148 hf: [[-1.1060255 ]\n",
      " [-1.0894995 ]\n",
      " [-0.6596224 ]\n",
      " [-0.15494782]\n",
      " [-0.49312514]\n",
      " [-0.49193358]\n",
      " [ 0.14394113]\n",
      " [ 0.11967592]]\n",
      "37 cost: 1.5092167 hf: [[-1.1059661 ]\n",
      " [-1.0894432 ]\n",
      " [-0.6595761 ]\n",
      " [-0.15491259]\n",
      " [-0.49308276]\n",
      " [-0.49189276]\n",
      " [ 0.14396685]\n",
      " [ 0.11970071]]\n",
      "38 cost: 1.5091184 hf: [[-1.1059065 ]\n",
      " [-1.0893872 ]\n",
      " [-0.65952975]\n",
      " [-0.15487725]\n",
      " [-0.49304026]\n",
      " [-0.491852  ]\n",
      " [ 0.1439926 ]\n",
      " [ 0.11972553]]\n",
      "39 cost: 1.5090202 hf: [[-1.1058469 ]\n",
      " [-1.0893312 ]\n",
      " [-0.6594834 ]\n",
      " [-0.15484202]\n",
      " [-0.49299788]\n",
      " [-0.49181122]\n",
      " [ 0.14401832]\n",
      " [ 0.11975032]]\n",
      "40 cost: 1.508922 hf: [[-1.1057875 ]\n",
      " [-1.0892749 ]\n",
      " [-0.659437  ]\n",
      " [-0.1548068 ]\n",
      " [-0.4929555 ]\n",
      " [-0.4917704 ]\n",
      " [ 0.14404404]\n",
      " [ 0.11977513]]\n",
      "41 cost: 1.5088238 hf: [[-1.1057279 ]\n",
      " [-1.0892189 ]\n",
      " [-0.6593906 ]\n",
      " [-0.15477145]\n",
      " [-0.49291307]\n",
      " [-0.49172956]\n",
      " [ 0.14406978]\n",
      " [ 0.11979993]]\n",
      "42 cost: 1.5087256 hf: [[-1.1056683 ]\n",
      " [-1.0891628 ]\n",
      " [-0.6593444 ]\n",
      " [-0.15473616]\n",
      " [-0.4928707 ]\n",
      " [-0.4916888 ]\n",
      " [ 0.1440955 ]\n",
      " [ 0.11982474]]\n",
      "43 cost: 1.5086274 hf: [[-1.1056087 ]\n",
      " [-1.0891066 ]\n",
      " [-0.6592981 ]\n",
      " [-0.15470088]\n",
      " [-0.49282825]\n",
      " [-0.49164796]\n",
      " [ 0.14412121]\n",
      " [ 0.11984953]]\n",
      "44 cost: 1.5085292 hf: [[-1.1055493 ]\n",
      " [-1.0890505 ]\n",
      " [-0.65925175]\n",
      " [-0.15466565]\n",
      " [-0.4927858 ]\n",
      " [-0.49160713]\n",
      " [ 0.14414695]\n",
      " [ 0.11987433]]\n",
      "45 cost: 1.508431 hf: [[-1.1054897 ]\n",
      " [-1.0889943 ]\n",
      " [-0.6592054 ]\n",
      " [-0.1546303 ]\n",
      " [-0.49274343]\n",
      " [-0.49156642]\n",
      " [ 0.1441727 ]\n",
      " [ 0.11989914]]\n",
      "46 cost: 1.5083328 hf: [[-1.1054301 ]\n",
      " [-1.0889382 ]\n",
      " [-0.6591591 ]\n",
      " [-0.15459502]\n",
      " [-0.492701  ]\n",
      " [-0.49152553]\n",
      " [ 0.14419842]\n",
      " [ 0.11992393]]\n",
      "47 cost: 1.5082346 hf: [[-1.1053708 ]\n",
      " [-1.088882  ]\n",
      " [-0.65911275]\n",
      " [-0.15455973]\n",
      " [-0.49265856]\n",
      " [-0.49148476]\n",
      " [ 0.14422417]\n",
      " [ 0.11994874]]\n",
      "48 cost: 1.5081363 hf: [[-1.1053112 ]\n",
      " [-1.088826  ]\n",
      " [-0.6590664 ]\n",
      " [-0.15452445]\n",
      " [-0.49261612]\n",
      " [-0.49144393]\n",
      " [ 0.14424989]\n",
      " [ 0.11997354]]\n",
      "49 cost: 1.5080383 hf: [[-1.1052518 ]\n",
      " [-1.0887699 ]\n",
      " [-0.6590201 ]\n",
      " [-0.15448916]\n",
      " [-0.49257368]\n",
      " [-0.49140316]\n",
      " [ 0.1442756 ]\n",
      " [ 0.11999835]]\n",
      "50 cost: 1.50794 hf: [[-1.1051922 ]\n",
      " [-1.0887136 ]\n",
      " [-0.65897375]\n",
      " [-0.15445387]\n",
      " [-0.4925313 ]\n",
      " [-0.49136233]\n",
      " [ 0.14430134]\n",
      " [ 0.12002315]]\n",
      "51 cost: 1.507842 hf: [[-1.1051326 ]\n",
      " [-1.0886576 ]\n",
      " [-0.6589274 ]\n",
      " [-0.15441865]\n",
      " [-0.49248892]\n",
      " [-0.49132156]\n",
      " [ 0.14432706]\n",
      " [ 0.12004796]]\n",
      "52 cost: 1.5077436 hf: [[-1.105073  ]\n",
      " [-1.0886014 ]\n",
      " [-0.6588811 ]\n",
      " [-0.15438336]\n",
      " [-0.49244642]\n",
      " [-0.49128067]\n",
      " [ 0.14435278]\n",
      " [ 0.12007275]]\n",
      "53 cost: 1.5076456 hf: [[-1.1050136 ]\n",
      " [-1.0885453 ]\n",
      " [-0.65883476]\n",
      " [-0.15434808]\n",
      " [-0.49240404]\n",
      " [-0.49123996]\n",
      " [ 0.14437853]\n",
      " [ 0.12009756]]\n",
      "54 cost: 1.5075476 hf: [[-1.104954  ]\n",
      " [-1.0884893 ]\n",
      " [-0.6587884 ]\n",
      " [-0.15431279]\n",
      " [-0.49236166]\n",
      " [-0.49119914]\n",
      " [ 0.14440426]\n",
      " [ 0.12012236]]\n",
      "55 cost: 1.5074492 hf: [[-1.1048944 ]\n",
      " [-1.088433  ]\n",
      " [-0.658742  ]\n",
      " [-0.1542775 ]\n",
      " [-0.49231923]\n",
      " [-0.4911583 ]\n",
      " [ 0.14443001]\n",
      " [ 0.12014717]]\n",
      "56 cost: 1.5073512 hf: [[-1.104835  ]\n",
      " [-1.088377  ]\n",
      " [-0.6586958 ]\n",
      " [-0.15424225]\n",
      " [-0.49227688]\n",
      " [-0.4911175 ]\n",
      " [ 0.1444557 ]\n",
      " [ 0.12017193]]\n",
      "57 cost: 1.5072529 hf: [[-1.1047755 ]\n",
      " [-1.0883209 ]\n",
      " [-0.65864956]\n",
      " [-0.15420699]\n",
      " [-0.49223447]\n",
      " [-0.49107683]\n",
      " [ 0.14448139]\n",
      " [ 0.12019671]]\n",
      "58 cost: 1.5071548 hf: [[-1.104716  ]\n",
      " [-1.0882648 ]\n",
      " [-0.6586032 ]\n",
      " [-0.15417174]\n",
      " [-0.49219206]\n",
      " [-0.49103597]\n",
      " [ 0.14450708]\n",
      " [ 0.12022148]]\n",
      "59 cost: 1.507057 hf: [[-1.1046565 ]\n",
      " [-1.0882089 ]\n",
      " [-0.6585569 ]\n",
      " [-0.15413648]\n",
      " [-0.4921497 ]\n",
      " [-0.49099523]\n",
      " [ 0.14453277]\n",
      " [ 0.12024625]]\n",
      "60 cost: 1.5069591 hf: [[-1.1045971 ]\n",
      " [-1.0881528 ]\n",
      " [-0.6585107 ]\n",
      " [-0.15410122]\n",
      " [-0.49210736]\n",
      " [-0.4909545 ]\n",
      " [ 0.14455846]\n",
      " [ 0.12027103]]\n",
      "61 cost: 1.506861 hf: [[-1.1045376 ]\n",
      " [-1.0880967 ]\n",
      " [-0.65846443]\n",
      " [-0.15406603]\n",
      " [-0.49206507]\n",
      " [-0.49091375]\n",
      " [ 0.14458415]\n",
      " [ 0.12029578]]\n",
      "62 cost: 1.506763 hf: [[-1.1044781 ]\n",
      " [-1.0880407 ]\n",
      " [-0.65841806]\n",
      " [-0.15403077]\n",
      " [-0.49202266]\n",
      " [-0.490873  ]\n",
      " [ 0.14460987]\n",
      " [ 0.12032054]]\n",
      "63 cost: 1.506665 hf: [[-1.1044188 ]\n",
      " [-1.0879848 ]\n",
      " [-0.65837187]\n",
      " [-0.15399557]\n",
      " [-0.4919803 ]\n",
      " [-0.4908322 ]\n",
      " [ 0.14463553]\n",
      " [ 0.12034529]]\n",
      "64 cost: 1.506567 hf: [[-1.1043593 ]\n",
      " [-1.0879285 ]\n",
      " [-0.6583257 ]\n",
      " [-0.15396038]\n",
      " [-0.49193797]\n",
      " [-0.49079147]\n",
      " [ 0.14466125]\n",
      " [ 0.12037005]]\n",
      "65 cost: 1.506469 hf: [[-1.1042998 ]\n",
      " [-1.0878726 ]\n",
      " [-0.6582793 ]\n",
      " [-0.15392512]\n",
      " [-0.49189562]\n",
      " [-0.4907508 ]\n",
      " [ 0.14468691]\n",
      " [ 0.12039481]]\n",
      "66 cost: 1.5063709 hf: [[-1.1042402 ]\n",
      " [-1.0878164 ]\n",
      " [-0.65823305]\n",
      " [-0.15388992]\n",
      " [-0.4918532 ]\n",
      " [-0.49071   ]\n",
      " [ 0.14471263]\n",
      " [ 0.12041956]]\n",
      "67 cost: 1.5062729 hf: [[-1.1041808 ]\n",
      " [-1.0877604 ]\n",
      " [-0.65818673]\n",
      " [-0.15385467]\n",
      " [-0.4918108 ]\n",
      " [-0.4906692 ]\n",
      " [ 0.14473829]\n",
      " [ 0.12044433]]\n",
      "68 cost: 1.5061749 hf: [[-1.1041213 ]\n",
      " [-1.0877044 ]\n",
      " [-0.65814054]\n",
      " [-0.15381941]\n",
      " [-0.49176845]\n",
      " [-0.49062845]\n",
      " [ 0.144764  ]\n",
      " [ 0.12046906]]\n",
      "69 cost: 1.5060769 hf: [[-1.1040618 ]\n",
      " [-1.0876484 ]\n",
      " [-0.65809417]\n",
      " [-0.15378416]\n",
      " [-0.4917261 ]\n",
      " [-0.4905877 ]\n",
      " [ 0.1447897 ]\n",
      " [ 0.12049381]]\n",
      "70 cost: 1.5059791 hf: [[-1.1040024 ]\n",
      " [-1.0875924 ]\n",
      " [-0.6580479 ]\n",
      " [-0.1537489 ]\n",
      " [-0.49168375]\n",
      " [-0.49054697]\n",
      " [ 0.14481539]\n",
      " [ 0.12051858]]\n",
      "71 cost: 1.5058811 hf: [[-1.1039429 ]\n",
      " [-1.0875363 ]\n",
      " [-0.6580017 ]\n",
      " [-0.1537137 ]\n",
      " [-0.49164134]\n",
      " [-0.49050623]\n",
      " [ 0.14484107]\n",
      " [ 0.12054333]]\n",
      "72 cost: 1.5057831 hf: [[-1.1038835 ]\n",
      " [-1.0874804 ]\n",
      " [-0.6579554 ]\n",
      " [-0.15367845]\n",
      " [-0.49159905]\n",
      " [-0.49046543]\n",
      " [ 0.14486673]\n",
      " [ 0.1205681 ]]\n",
      "73 cost: 1.5056851 hf: [[-1.1038239 ]\n",
      " [-1.0874243 ]\n",
      " [-0.65790915]\n",
      " [-0.15364313]\n",
      " [-0.4915567 ]\n",
      " [-0.49042475]\n",
      " [ 0.14489245]\n",
      " [ 0.12059285]]\n",
      "74 cost: 1.5055872 hf: [[-1.1037644 ]\n",
      " [-1.0873682 ]\n",
      " [-0.6578629 ]\n",
      " [-0.153608  ]\n",
      " [-0.49151435]\n",
      " [-0.490384  ]\n",
      " [ 0.14491814]\n",
      " [ 0.1206176 ]]\n",
      "75 cost: 1.5054891 hf: [[-1.1037049 ]\n",
      " [-1.0873122 ]\n",
      " [-0.6578166 ]\n",
      " [-0.15357274]\n",
      " [-0.4914719 ]\n",
      " [-0.49034315]\n",
      " [ 0.14494383]\n",
      " [ 0.12064236]]\n",
      "76 cost: 1.5053912 hf: [[-1.1036456 ]\n",
      " [-1.0872562 ]\n",
      " [-0.6577703 ]\n",
      " [-0.15353748]\n",
      " [-0.4914296 ]\n",
      " [-0.49030247]\n",
      " [ 0.14496952]\n",
      " [ 0.12066711]]\n",
      "77 cost: 1.5052934 hf: [[-1.1035861 ]\n",
      " [-1.0872    ]\n",
      " [-0.657724  ]\n",
      " [-0.15350229]\n",
      " [-0.49138725]\n",
      " [-0.49026167]\n",
      " [ 0.14499521]\n",
      " [ 0.12069187]]\n",
      "78 cost: 1.5051954 hf: [[-1.1035266 ]\n",
      " [-1.087144  ]\n",
      " [-0.65767777]\n",
      " [-0.15346697]\n",
      " [-0.4913449 ]\n",
      " [-0.49022093]\n",
      " [ 0.1450209 ]\n",
      " [ 0.12071663]]\n",
      "79 cost: 1.5050975 hf: [[-1.1034672 ]\n",
      " [-1.0870881 ]\n",
      " [-0.6576316 ]\n",
      " [-0.15343177]\n",
      " [-0.4913025 ]\n",
      " [-0.4901802 ]\n",
      " [ 0.14504659]\n",
      " [ 0.12074138]]\n",
      "80 cost: 1.5049996 hf: [[-1.1034077 ]\n",
      " [-1.0870321 ]\n",
      " [-0.6575854 ]\n",
      " [-0.15339652]\n",
      " [-0.49126014]\n",
      " [-0.4901394 ]\n",
      " [ 0.14507228]\n",
      " [ 0.12076615]]\n",
      "81 cost: 1.5049015 hf: [[-1.1033483 ]\n",
      " [-1.0869759 ]\n",
      " [-0.657539  ]\n",
      " [-0.15336132]\n",
      " [-0.49121773]\n",
      " [-0.4900987 ]\n",
      " [ 0.14509797]\n",
      " [ 0.1207909 ]]\n",
      "82 cost: 1.5048037 hf: [[-1.1032887 ]\n",
      " [-1.08692   ]\n",
      " [-0.65749264]\n",
      " [-0.15332606]\n",
      " [-0.49117544]\n",
      " [-0.49005798]\n",
      " [ 0.14512366]\n",
      " [ 0.12081565]]\n",
      "83 cost: 1.5047059 hf: [[-1.1032293 ]\n",
      " [-1.086864  ]\n",
      " [-0.65744656]\n",
      " [-0.15329081]\n",
      " [-0.4911331 ]\n",
      " [-0.49001718]\n",
      " [ 0.14514935]\n",
      " [ 0.1208404 ]]\n",
      "84 cost: 1.5046079 hf: [[-1.1031698 ]\n",
      " [-1.0868078 ]\n",
      " [-0.65740037]\n",
      " [-0.15325561]\n",
      " [-0.49109074]\n",
      " [-0.48997644]\n",
      " [ 0.14517504]\n",
      " [ 0.12086515]]\n",
      "85 cost: 1.5045099 hf: [[-1.1031103 ]\n",
      " [-1.0867518 ]\n",
      " [-0.657354  ]\n",
      " [-0.15322036]\n",
      " [-0.49104834]\n",
      " [-0.4899357 ]\n",
      " [ 0.14520076]\n",
      " [ 0.12088992]]\n",
      "86 cost: 1.5044117 hf: [[-1.1030508 ]\n",
      " [-1.0866957 ]\n",
      " [-0.6573076 ]\n",
      " [-0.15318516]\n",
      " [-0.491006  ]\n",
      " [-0.4898949 ]\n",
      " [ 0.14522642]\n",
      " [ 0.12091467]]\n",
      "87 cost: 1.504314 hf: [[-1.1029913 ]\n",
      " [-1.0866399 ]\n",
      " [-0.65726143]\n",
      " [-0.15314996]\n",
      " [-0.49096364]\n",
      " [-0.48985422]\n",
      " [ 0.14525214]\n",
      " [ 0.12093942]]\n",
      "88 cost: 1.5042162 hf: [[-1.102932  ]\n",
      " [-1.0865837 ]\n",
      " [-0.65721524]\n",
      " [-0.15311465]\n",
      " [-0.49092123]\n",
      " [-0.48981342]\n",
      " [ 0.14527778]\n",
      " [ 0.12096418]]\n",
      "89 cost: 1.5041182 hf: [[-1.1028724 ]\n",
      " [-1.0865277 ]\n",
      " [-0.657169  ]\n",
      " [-0.15307945]\n",
      " [-0.49087894]\n",
      " [-0.48977268]\n",
      " [ 0.14530347]\n",
      " [ 0.12098894]]\n",
      "90 cost: 1.5040202 hf: [[-1.1028129 ]\n",
      " [-1.0864717 ]\n",
      " [-0.6571226 ]\n",
      " [-0.1530442 ]\n",
      " [-0.4908366 ]\n",
      " [-0.48973194]\n",
      " [ 0.14532916]\n",
      " [ 0.1210137 ]]\n",
      "91 cost: 1.5039225 hf: [[-1.1027534 ]\n",
      " [-1.0864158 ]\n",
      " [-0.6570763 ]\n",
      " [-0.153009  ]\n",
      " [-0.49079424]\n",
      " [-0.4896912 ]\n",
      " [ 0.14535484]\n",
      " [ 0.12103845]]\n",
      "92 cost: 1.5038246 hf: [[-1.1026942 ]\n",
      " [-1.0863596 ]\n",
      " [-0.6570301 ]\n",
      " [-0.15297371]\n",
      " [-0.49075186]\n",
      " [-0.4896505 ]\n",
      " [ 0.14538053]\n",
      " [ 0.12106317]]\n",
      "93 cost: 1.503727 hf: [[-1.1026347 ]\n",
      " [-1.0863037 ]\n",
      " [-0.6569839 ]\n",
      " [-0.15293854]\n",
      " [-0.4907096 ]\n",
      " [-0.48960978]\n",
      " [ 0.14540616]\n",
      " [ 0.12108791]]\n",
      "94 cost: 1.5036292 hf: [[-1.1025753 ]\n",
      " [-1.0862478 ]\n",
      " [-0.65693784]\n",
      " [-0.15290332]\n",
      " [-0.49066722]\n",
      " [-0.48956907]\n",
      " [ 0.14543185]\n",
      " [ 0.12111263]]\n",
      "95 cost: 1.5035312 hf: [[-1.1025159 ]\n",
      " [-1.0861917 ]\n",
      " [-0.6568915 ]\n",
      " [-0.15286815]\n",
      " [-0.4906249 ]\n",
      " [-0.48952836]\n",
      " [ 0.14545748]\n",
      " [ 0.12113735]]\n",
      "96 cost: 1.5034336 hf: [[-1.1024565 ]\n",
      " [-1.0861357 ]\n",
      " [-0.6568452 ]\n",
      " [-0.15283293]\n",
      " [-0.4905826 ]\n",
      " [-0.48948765]\n",
      " [ 0.14548317]\n",
      " [ 0.12116209]]\n",
      "97 cost: 1.5033356 hf: [[-1.102397  ]\n",
      " [-1.0860796 ]\n",
      " [-0.65679914]\n",
      " [-0.15279776]\n",
      " [-0.4905404 ]\n",
      " [-0.489447  ]\n",
      " [ 0.14550881]\n",
      " [ 0.12118681]]\n",
      "98 cost: 1.503238 hf: [[-1.1023376 ]\n",
      " [-1.0860237 ]\n",
      " [-0.6567528 ]\n",
      " [-0.15276259]\n",
      " [-0.490498  ]\n",
      " [-0.48940623]\n",
      " [ 0.14553447]\n",
      " [ 0.12121153]]\n",
      "99 cost: 1.5031402 hf: [[-1.1022782 ]\n",
      " [-1.0859678 ]\n",
      " [-0.65670663]\n",
      " [-0.15272743]\n",
      " [-0.4904557 ]\n",
      " [-0.48936552]\n",
      " [ 0.14556013]\n",
      " [ 0.12123625]]\n",
      "100 cost: 1.5030422 hf: [[-1.1022187 ]\n",
      " [-1.0859118 ]\n",
      " [-0.65666044]\n",
      " [-0.1526922 ]\n",
      " [-0.4904133 ]\n",
      " [-0.4893248 ]\n",
      " [ 0.14558578]\n",
      " [ 0.12126097]]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(101):\n",
    "    cv,hv,_=sess.run([cost,hf,train], feed_dict={x:xdata, y:ydata})\n",
    "    print(step,\"cost:\",cv, \"hf:\",hv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "#y값은 one hot 인코딩 방식으로 초기화 함...(a,b,c)중 하나를 hot하게 함...\n",
    "y_data = [[0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [1, 0, 0],#0\n",
    "          [1, 0, 0]]#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=3\n",
    "x=tf.placeholder(\"float\",[None,4])\n",
    "y=tf.placeholder(\"float\",[None,num_classes])\n",
    "w=tf.Variable(tf.random_normal([4,num_classes]))\n",
    "b=tf.Variable(tf.random_normal([num_classes]))\n",
    "#hf=x*w+b\n",
    "#[None,4],[4,3]+[3]="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf=tf.nn.softmax(tf.matmul(x,w)+b) # 합이 1인 확률로 출력하기위해 softmax 함수 적용\n",
    "cost= tf.reduce_mean(-tf.reduce_sum(y*tf.log(hf), axis=1)) \n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6480659\n",
      "200 0.506062\n",
      "400 0.4128444\n",
      "600 0.33731052\n",
      "800 0.2632463\n",
      "1000 0.22669907\n",
      "1200 0.2056907\n",
      "1400 0.1881704\n",
      "1600 0.17332926\n",
      "1800 0.16059835\n",
      "2000 0.1495614\n",
      "==================================================\n",
      "[1]\n",
      "==================================================\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# with 구문을 벗어나면 session 객체가 해제됨\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict={x:x_data, y:y_data})\n",
    "        if step %200 == 0:\n",
    "            print(step, sess.run(cost,feed_dict={x:x_data, y:y_data}))\n",
    "    print(\"=\"*50)\n",
    "# xdata= 1,11,7,9 => y?\n",
    "    res=sess.run(hf, feed_dict={x:[[1,11,7,9]]})\n",
    "    # 본래의 x데이터가 2차원이므로 xdata를 2차원으로 써줘야함\n",
    "    print(sess.run(tf.argmax(res,axis=1)))\n",
    "    print(\"=\"*50)\n",
    "    res=sess.run(hf, feed_dict={x:[[1,11,7,9],\n",
    "                                   [1,3,4,3],\n",
    "                                   [1,1,0,1]]})\n",
    "    print(sess.run(tf.argmax(res,axis=1))) # 함수를 최대값으로 만들기 위한 인덱스 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16)\n",
      "(101, 1)\n"
     ]
    }
   ],
   "source": [
    "xy=np.loadtxt(\"data/zoo.csv\", delimiter=',', dtype=np.float32)\n",
    "xy.shape #(101, 17)\n",
    "xdata=xy[:,0:-1]\n",
    "ydata=xy[:,[-1]]\n",
    "print(xdata.shape)\n",
    "print(ydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_5:0\", shape=(?, 1, 7), dtype=float32)\n",
      "one_hot after reshape Tensor(\"Reshape_2:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32, [None,16])\n",
    "y=tf.placeholder(tf.int32, [None,1]) \n",
    "y_one_hot=tf.one_hot(y,7) # (원핫인코딩할 변수, 분류되어야할 종류), ex) 동물이 3종류 : 3(y) -> 0001000(y_one_hot)\n",
    "print(\"one_hot\", y_one_hot)\n",
    "y_one_hot=tf.reshape(y_one_hot,[-1,7])\n",
    "print(\"one_hot after reshape\", y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot 함수는 한 차원 높게 변환됨  \n",
    "y : [[0],[3],....,[5]] => [None,1] 의 shape  \n",
    "one_hot => [[[1000000]],[[0001000]],....,[[00000100]]] => [None,1,7]  \n",
    "우리가 바라는 출력 결과의 모습은 [None,7] 임  \n",
    "이를 위해서, reshape 을 하면 됨 tf.shape(y_one_hot,[-1,7]) 코딩하면 [None,7]로 reshape 됨  \n",
    "[[[1000000]],[[0001000]]]...] => [[1000000],[0001000]....]  \n",
    "\n",
    "y : 0~6 (7가지 종류의 동물) -> 분류기 7개  \n",
    "w1x1+w2x2+w3x3...w7x7+b => softmax -> 0번 종류의 동물에 대한 확률  \n",
    "w1x1+w2x2+w3x3...w7x7+b => softmax -> 1번 종류의 동물에 대한 확률  \n",
    "....  \n",
    "w1x1+w2x2+w3x3...w7x7+b => softmax -> 6번 종류의 동물에 대한 확률  \n",
    "편하게 쓰기 위해 매트릭스 활용  \n",
    "y데이터가 원핫인코딩되어 있는 경우, 1000000, 0100000... => [None,7]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=tf.Variable(tf.random_normal([16,7]))\n",
    "b=tf.Variable(tf.random_normal([7]))\n",
    "# x:[None,16], w:[16,7]\n",
    "logit=tf.matmul(x,w)+b\n",
    "hf=tf.nn.softmax(logit) \n",
    "cost=tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y_one_hot)\n",
    "cost2=tf.reduce_mean(cost)\n",
    "train=tf.train.GradientDescentOptimizer(0.1).minimize(cost2)\n",
    "prediction=tf.argmax(hf,1) # 최대값에 해당하는 인덱스가 출력\n",
    "# 예측이 실제값과 잘 맞는지 노드 정의\n",
    "correct_prediction=tf.equal(prediction,tf.argmax(y_one_hot,1))\n",
    "# 정확도를 수치로 표현\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 cost: 5.9643197 acc: 0.0990099\n",
      "step: 100 cost: 0.6303851 acc: 0.8217822\n",
      "step: 200 cost: 0.41774717 acc: 0.8811881\n",
      "step: 300 cost: 0.3232745 acc: 0.9108911\n",
      "step: 400 cost: 0.26422137 acc: 0.9207921\n",
      "step: 500 cost: 0.22188602 acc: 0.9405941\n",
      "step: 600 cost: 0.18959822 acc: 0.9405941\n",
      "step: 700 cost: 0.16425446 acc: 0.95049506\n",
      "step: 800 cost: 0.14406475 acc: 0.980198\n",
      "step: 900 cost: 0.12780881 acc: 0.980198\n",
      "step: 1000 cost: 0.11458009 acc: 0.980198\n",
      "step: 1100 cost: 0.10369035 acc: 0.990099\n",
      "step: 1200 cost: 0.09461895 acc: 0.990099\n",
      "step: 1300 cost: 0.086974174 acc: 0.990099\n",
      "step: 1400 cost: 0.080460854 acc: 1.0\n",
      "step: 1500 cost: 0.07485528 acc: 1.0\n",
      "step: 1600 cost: 0.069986284 acc: 1.0\n",
      "step: 1700 cost: 0.06572138 acc: 1.0\n",
      "step: 1800 cost: 0.061956957 acc: 1.0\n",
      "step: 1900 cost: 0.05861109 acc: 1.0\n",
      "step: 2000 cost: 0.05561837 acc: 1.0\n",
      "step: 2100 cost: 0.05292597 acc: 1.0\n",
      "step: 2200 cost: 0.050490987 acc: 1.0\n",
      "step: 2300 cost: 0.048278067 acc: 1.0\n",
      "step: 2400 cost: 0.04625815 acc: 1.0\n",
      "step: 2500 cost: 0.044406857 acc: 1.0\n",
      "step: 2600 cost: 0.042703714 acc: 1.0\n",
      "step: 2700 cost: 0.041131496 acc: 1.0\n",
      "step: 2800 cost: 0.039675534 acc: 1.0\n",
      "step: 2900 cost: 0.038323194 acc: 1.0\n",
      "step: 3000 cost: 0.037063718 acc: 1.0\n",
      "step: 3100 cost: 0.035887655 acc: 1.0\n",
      "step: 3200 cost: 0.034786925 acc: 1.0\n",
      "step: 3300 cost: 0.03375438 acc: 1.0\n",
      "step: 3400 cost: 0.032783814 acc: 1.0\n",
      "step: 3500 cost: 0.0318697 acc: 1.0\n",
      "step: 3600 cost: 0.031007193 acc: 1.0\n",
      "step: 3700 cost: 0.03019196 acc: 1.0\n",
      "step: 3800 cost: 0.029420193 acc: 1.0\n",
      "step: 3900 cost: 0.028688416 acc: 1.0\n",
      "step: 4000 cost: 0.027993536 acc: 1.0\n",
      "step: 4100 cost: 0.02733283 acc: 1.0\n",
      "step: 4200 cost: 0.026703747 acc: 1.0\n",
      "step: 4300 cost: 0.02610409 acc: 1.0\n",
      "step: 4400 cost: 0.025531735 acc: 1.0\n",
      "step: 4500 cost: 0.024984917 acc: 1.0\n",
      "step: 4600 cost: 0.024461864 acc: 1.0\n",
      "step: 4700 cost: 0.023961047 acc: 1.0\n",
      "step: 4800 cost: 0.023481114 acc: 1.0\n",
      "step: 4900 cost: 0.023020664 acc: 1.0\n",
      "step: 5000 cost: 0.022578584 acc: 1.0\n",
      "step: 5100 cost: 0.022153754 acc: 1.0\n",
      "step: 5200 cost: 0.021745179 acc: 1.0\n",
      "step: 5300 cost: 0.021351932 acc: 1.0\n",
      "step: 5400 cost: 0.02097318 acc: 1.0\n",
      "step: 5500 cost: 0.020608017 acc: 1.0\n",
      "step: 5600 cost: 0.020255841 acc: 1.0\n",
      "step: 5700 cost: 0.019915866 acc: 1.0\n",
      "step: 5800 cost: 0.019587496 acc: 1.0\n",
      "step: 5900 cost: 0.01927013 acc: 1.0\n",
      "step: 6000 cost: 0.018963201 acc: 1.0\n",
      "step: 6100 cost: 0.018666247 acc: 1.0\n",
      "step: 6200 cost: 0.018378746 acc: 1.0\n",
      "step: 6300 cost: 0.018100249 acc: 1.0\n",
      "step: 6400 cost: 0.017830329 acc: 1.0\n",
      "step: 6500 cost: 0.017568588 acc: 1.0\n",
      "step: 6600 cost: 0.017314643 acc: 1.0\n",
      "step: 6700 cost: 0.017068168 acc: 1.0\n",
      "step: 6800 cost: 0.01682883 acc: 1.0\n",
      "step: 6900 cost: 0.01659632 acc: 1.0\n",
      "step: 7000 cost: 0.016370349 acc: 1.0\n",
      "step: 7100 cost: 0.016150635 acc: 1.0\n",
      "step: 7200 cost: 0.015936889 acc: 1.0\n",
      "step: 7300 cost: 0.015728908 acc: 1.0\n",
      "step: 7400 cost: 0.015526439 acc: 1.0\n",
      "step: 7500 cost: 0.0153292455 acc: 1.0\n",
      "step: 7600 cost: 0.015137151 acc: 1.0\n",
      "step: 7700 cost: 0.0149499765 acc: 1.0\n",
      "step: 7800 cost: 0.0147674605 acc: 1.0\n",
      "step: 7900 cost: 0.014589509 acc: 1.0\n",
      "step: 8000 cost: 0.01441591 acc: 1.0\n",
      "step: 8100 cost: 0.014246482 acc: 1.0\n",
      "step: 8200 cost: 0.014081135 acc: 1.0\n",
      "step: 8300 cost: 0.013919658 acc: 1.0\n",
      "step: 8400 cost: 0.013761949 acc: 1.0\n",
      "step: 8500 cost: 0.013607852 acc: 1.0\n",
      "step: 8600 cost: 0.013457271 acc: 1.0\n",
      "step: 8700 cost: 0.013310113 acc: 1.0\n",
      "step: 8800 cost: 0.013166182 acc: 1.0\n",
      "step: 8900 cost: 0.0130254 acc: 1.0\n",
      "step: 9000 cost: 0.012887681 acc: 1.0\n",
      "step: 9100 cost: 0.012752933 acc: 1.0\n",
      "step: 9200 cost: 0.012621037 acc: 1.0\n",
      "step: 9300 cost: 0.01249192 acc: 1.0\n",
      "step: 9400 cost: 0.012365481 acc: 1.0\n",
      "step: 9500 cost: 0.012241634 acc: 1.0\n",
      "step: 9600 cost: 0.012120312 acc: 1.0\n",
      "step: 9700 cost: 0.012001416 acc: 1.0\n",
      "step: 9800 cost: 0.0118849045 acc: 1.0\n",
      "step: 9900 cost: 0.011770686 acc: 1.0\n",
      "step: 10000 cost: 0.011658671 acc: 1.0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 4 실제 y: 4\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 2 실제 y: 2\n",
      "[True] 예측: 3 실제 y: 3\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 1 실제 y: 1\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 5 실제 y: 5\n",
      "[True] 예측: 0 실제 y: 0\n",
      "[True] 예측: 6 실제 y: 6\n",
      "[True] 예측: 1 실제 y: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict={x:xdata,y:ydata})\n",
    "        if step % 100 ==0:\n",
    "            cv,av=sess.run([cost2, accuracy], feed_dict={x:xdata,y:ydata})\n",
    "            print(\"step:\",step,\"cost:\",cv,\"acc:\",av)\n",
    "\n",
    "# 전체 데이터로 트레이닝 수행하여 모델 생성\n",
    "# 모델에 전체 데이터를 집어 넣어서 정확도 출력\n",
    "# 데이터를 분할하지 않았음\n",
    "    pred=sess.run(prediction, feed_dict={x:xdata})\n",
    "    for p,y in zip(pred, ydata.flatten()):\n",
    "          print(\"[{}] 예측: {} 실제 y: {}\".format(p==int(y),p,int(y)))\n",
    "    \n",
    "    # print(sess.run(prediction, feed_dict={x:[[...]]}))\n",
    "    # [[0],[3],....] => [0,3,...]\n",
    "    # print(pred) # 우리 모델에서 예측한 클래스 \n",
    "    \n",
    "    # flatten : 차원을 한차원 내려주는 역할\n",
    "    # [[1],[0]] -> [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습문제\n",
    "1. mushroom 데이터 분류 모델\n",
    "2. animal 데이터를 7:3로 분할 -> 모델, 테스트\n",
    "3. breask cancer(유방암) 분류기\n",
    "4. wine quality 분류기 (10단계) \n",
    "데이터:https://archive.ics.uci.edu/ml/datasets.php?format=&task=&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
