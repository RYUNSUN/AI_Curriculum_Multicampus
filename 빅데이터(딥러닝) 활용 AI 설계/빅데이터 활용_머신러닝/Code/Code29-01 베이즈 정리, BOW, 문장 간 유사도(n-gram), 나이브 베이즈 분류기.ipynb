{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  스팸필터\n",
    "    -홍길동 받은 메일의 80% 스팸 메일  \n",
    "    -스팸 메일의 95%에서 '대출'이라는 단어  \n",
    "    -정상메일의 2%에서 '대출 단어  \n",
    "    ---------- 주어진 데이터 -----------  \n",
    "    문제) 방금 메일을 받음.  \n",
    "    '대출'이라는 단어가 제목에서 발견됨.  \n",
    "    스팸일 확률은?  \n",
    "    P(스팸|대출)=P(대출|스팸)P(스팸) / P(대출)  \n",
    "    =0.95*0.8 / P(대출)  \n",
    "    P(대출)=P(대출|스팸)P(스팸) + P(대출|정상)P(정상)  \n",
    "          = 0.95 * 0.8 + 0.02 * 0.2 = 0.764   \n",
    "    P(스팸|대출) = 0.95 * 0.8 / 0.764 = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 스팸 메일 (나이브 베이지안 필터기)\n",
    "    - P(A), P(B), P(B|A), P(A|B)\n",
    "    - P(A|B) = P(B|A)P(A) / P(B)\n",
    "    - P(정상|텍스트), P(비정상|텍스트)\n",
    "    - P(정상|텍스트) = (P(정상|텍스트)*P(정상메일)) / P(텍스트)\n",
    "    - P(스팸|텍스트) = (P(텍스트|스팸)*P(스팸)) / P(텍스트)\n",
    "    => 단어가 3개(단어는 서로 독립)\n",
    "    - 나이브 베이즈 분류기\n",
    "        P(정상|텍스트) = P(w1|정상메일)*P(w2|정상메일)*P(w3|정상메일)=P(w1,w2,w3|정상메일)\n",
    "        P(스팸|텍스트) = P(w1|스팸)*P(w2|스팸)*P(w3|스팸)\n",
    "        단어 순서 고려 x\n",
    "    - 만약 텍스트 있는 단어 중 어떤 단어가 받은 메일에 없으면 전체 확률 값이 0이 되므로 이를 방지하기 위해 +1을 해주는데 이를, '라플라스 스무딩'이라고 함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words(BOW) : 단어들의 등장 횟수로 표현(단어가방)\n",
    "    1) 주어진 단어에 대해 고유의 인덱스 부여\n",
    "    2) 단어의 등장 횟수 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 금요일입니다 내일은 토요일입니다 다음주 화요일에는 특강이 있습니다'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "token=\"오늘은 금요일입니다. 내일은 토요일입니다. 다음주 화요일에는 특강이 있습니다\"\n",
    "# 정규표현식을 사용하여 token에 지정되 문자 중에서 (.)을 제거하세요.\n",
    "token=re.sub('[.]','',token)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '은',\n",
       " '금요일',\n",
       " '입니다',\n",
       " '내일',\n",
       " '은',\n",
       " '토요일',\n",
       " '입니다',\n",
       " '다음주',\n",
       " '화요일',\n",
       " '에는',\n",
       " '특강',\n",
       " '이',\n",
       " '있습니다']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token=okt.morphs(token)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'오늘': 1, '은': 2, '금요일': 1, '입니다': 2, '내일': 1, '토요일': 1, '다음주': 1, '화요일': 1, '에는': 1, '특강': 1, '이': 1, '있습니다': 1}\n"
     ]
    }
   ],
   "source": [
    "# 1) 주어진 단어에 대해 고유의 인덱스 부여(0~11)\n",
    "# 2) 단어의 등장 횟수 벡터 생성\n",
    "# 1) => {'오늘':0,....'있습니다':11}\n",
    "# 2) => [1,2,1,1,1,2,1,1,1,....]\n",
    "token_dic={}\n",
    "for w in token_list:\n",
    "    if w not in token_dic:\n",
    "        token_dic[w] = 1\n",
    "    else :\n",
    "        token_dic[w] += 1\n",
    "print(token_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'오늘': 0, '은': 1, '금요일': 2, '입니다': 3, '내일': 4, '토요일': 5, '다음주': 6, '화요일': 7, '에는': 8, '특강': 9, '이': 10, '있습니다': 11}\n"
     ]
    }
   ],
   "source": [
    "word2index={}\n",
    "bow=[]\n",
    "for voc in token:\n",
    "    if voc not in word2index.keys():\n",
    "        word2index[voc]=len(word2index)\n",
    "        bow.insert(len(word2index)-1,1)\n",
    "    else:\n",
    "        index=word2index.get(voc)\n",
    "        bow[index]=bow[index]+1\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 7, 1, 3, 4, 2, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1]]\n",
      "{'know': 0, 'want': 2, 'love': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=['you know I want your love. because I love you.']\n",
    "# vec=CountVectorizer() #BOW 생성 클래스\n",
    "# vec=CountVectorizer(stop_words=['I']) # 불용어 사용(사용자 정의)\n",
    "vec=CountVectorizer(stop_words='english') #불용어 사용(사용자 정의)\n",
    "print(vec.fit_transform(text).toarray()) #각 단어별 개수를 세며, 변환\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 문장간 유사도 조사(N-gram)\n",
    "    (1, target) \n",
    "    오늘 역삼에서 맛있는 돈가스를 먹었다.\n",
    "    (2, source) \n",
    "    역삼에서 먹었던 오늘의 돈가스는 맛있었다.\n",
    "    (2)번 원문장에 대해 (1)번 대상문장이 어느정도 유사한지 출력?\n",
    "    (1)번 문장에 대해 n=2로 하여 문장 분리하면\n",
    "    => '오늘','늘','역','역삼',...,'다.'\n",
    "    => 만약 길이가 20이라 가정하고, \n",
    "    (2)번 원문장에 대해서도 n=2로 하여 문장 분리했을 때, \n",
    "    공통으로 존재하는 단어가 5개 존재한다면, \n",
    "    5/20 = 25% 유사도를 갖는다. \n",
    "    \n",
    "    두 문장의 유사도? (using n-gram)\n",
    "    ex) n=2\n",
    "    (1) '오늘', '늘', '역', '역삼',...'다.' => 20개 중 5개가 (2)과 일치\n",
    "    (2) '역삼','삼에','에서',....'다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"오늘 역삼에서 맛있는 돈가스를 먹었다.\"\n",
    "source=\"역삼에서 먹었던 오늘의 돈가스는 맛있었다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram: 0.75 ['오늘', '역삼', '삼에', '에서', '서 ', ' 맛', '맛있', '는 ', ' 돈', '돈가', '가스', ' 먹', '먹었', '었다', '다.']\n",
      "3-gram: 0.42105263157894735 ['역삼에', '삼에서', '에서 ', ' 맛있', ' 돈가', '돈가스', ' 먹었', '었다.']\n"
     ]
    }
   ],
   "source": [
    "def ngram(s,num):\n",
    "    res=[]\n",
    "    slen=len(s)-num+1\n",
    "    for i in range(slen):\n",
    "        ss=s[i:i+num]\n",
    "        res.append(ss)\n",
    "    return res\n",
    "\n",
    "def diff_ngram(sa, sb, num):\n",
    "    a = ngram(sa, num)\n",
    "    b = ngram(sb, num)\n",
    "    r=[]\n",
    "    cnt=0\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            if i == j:\n",
    "                cnt += 1\n",
    "                r.append(i)\n",
    "    return cnt/len(a), r\n",
    "\n",
    "r2, word2 = diff_ngram(target,source,2)\n",
    "r3, word3 = diff_ngram(target,source,3)\n",
    "print('2-gram:', r2, word2)\n",
    "print('3-gram:', r3, word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus=['you know I want your love',\n",
    "       'I like you',\n",
    "       'what should i do']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "# DTM 생성\n",
    "vec=CountVectorizer()\n",
    "print(vec.fit_transform(corpus).toarray())\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "==================================================\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv=TfidfVectorizer().fit(corpus)\n",
    "print(\"=\"*50)\n",
    "print(tfidfv.fit_transform(corpus).toarray())\n",
    "print(\"=\"*50)\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이브 베이즈 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "11314 11314 20\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# 2만여편의 뉴스데이터\n",
    "newsdata=fetch_20newsgroups(subset=\"train\")\n",
    "print(newsdata.keys())\n",
    "print(len(newsdata.data), len(newsdata.filenames), len(newsdata.target_names))\n",
    "print(newsdata.target_names) # 20개의 뉴스 그룹 확인\n",
    "print(newsdata.target[0]) #0번 뉴스가 7번 카테고리에 해당한다는 의미(rec.autos)\n",
    "print(newsdata.target_names[7]) # 20개의 그룹 중 7번째"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "4\n",
      "comp.sys.mac.hardware\n"
     ]
    }
   ],
   "source": [
    "print(newsdata.data[1]) # 뉴스 데이터의 1번째 뉴스 \n",
    "print(newsdata.target[1]) #4\n",
    "print(newsdata.target_names[4]) #comp.sys.mac.hardware : 하드웨어에 속하는 글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "7532\n"
     ]
    }
   ],
   "source": [
    "testdata=fetch_20newsgroups(subset=\"test\")\n",
    "print(testdata.keys())\n",
    "print(len(testdata.data)) #7532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "alldata=fetch_20newsgroups(subset=\"test\")\n",
    "print(alldata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 텍스트 -> bow 생성 (bow에 등장하지 않은 단어는 OOV)\n",
    "# 테스트 단어를 분류기 넣었을 때 만약 BOW에 없는 단어인 경우 라플라스 스무딩에 의해 +1이 됨\n",
    "# 보통 분류기 생성 전, dtm을 tf-idf 행렬로 변환줌(성능이 훨씬 올라감)\n",
    "dtmVector=CountVectorizer()\n",
    "xtrainDtm=dtmVector.fit_transform(newsdata.data)\n",
    "xtrainDtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. dtm -> tf-idf 행렬 변환\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # dtm을 ti-idf 행렬로 변환해주는 클래스\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "tfidfv=tfidf_transformer.fit_transform(xtrainDtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 나이브베이즈 분류기 모델 생성\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(tfidfv, newsdata.target) # alpha:라플라스 스무딩 값, train 데이터와 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=fetch_20newsgroups(subset='test',shuffle=True)\n",
    "xtestDtm=dtmVector.transform(testdata.data)\n",
    "tfidf_test=tfidf_transformer.transform(xtestDtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted=model.predict(tfidf_test)\n",
    "print(\"정확도:\", accuracy_score(testdata.target, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
